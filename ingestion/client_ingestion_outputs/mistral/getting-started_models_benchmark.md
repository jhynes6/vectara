---
source: "website"
content_type: "other"
url: "https://docs.mistral.ai/getting-started/models/benchmark/"
title: "/getting-started/models/benchmark/"
domain: "docs.mistral.ai"
path: "/getting-started/models/benchmark/"
scraped_time: "2025-09-08T18:09:58.298268"
url_depth: 3
word_count: 1142
---

Models Benchmarks | Mistral AI

[Skip to main content](#__docusaurus_skipToContent_fallback)

[

![Mistral AI Logo](/img/logo.svg)![Mistral AI Logo](/img/logo-dark.svg)

](https://mistral.ai/)[Le Chat](https://chat.mistral.ai/)[La Plateforme](https://console.mistral.ai/)[Docs](/)[Cookbooks (beta)](/cookbooks/)[API](/api/)

[GitHub](https://github.com/mistralai/)[Discord](https://discord.gg/mistralai)

*   Getting Started
*   [Introduction](/)
*   [Quickstart](/getting-started/quickstart/)
*   [Models](/getting-started/models/models_overview/)

*   [Models Benchmarks](/getting-started/models/benchmark/)
*   [Model selection](/getting-started/models/picking/)
*   [Model weights](/getting-started/models/weights/)
*   [SDK Clients](/getting-started/clients/)
*   [Model customization](/getting-started/customization/)
*   [Developer examples](/getting-started/stories/)
*   [Changelog](/getting-started/changelog/)
*   [Glossary](/getting-started/glossary/)
*   Capabilities
*   [Text and Chat Completions](/capabilities/completion/)
*   [Vision](/capabilities/vision/)
*   [Audio & Transcription](/capabilities/audio/)
*   [Reasoning](/capabilities/reasoning/)
*   [Document AI](/capabilities/document_ai/document_ai_overview/)

*   [Coding](/capabilities/code_generation/)
*   [Embeddings](/capabilities/embeddings/overview/)

*   [Function calling](/capabilities/function_calling/)
*   [Citations and References](/capabilities/citations/)
*   [Structured Output](/capabilities/structured-output/structured_output_overview/)

*   [Moderation](/capabilities/guardrailing/)
*   [Finetuning](/capabilities/finetuning/finetuning_overview/)

*   [Batch Inference](/capabilities/batch/)
*   [Predicted outputs](/capabilities/predicted-outputs/)
*   Agents
*   [Agents Introduction](/agents/agents_introduction/)
*   [Agents & Conversations](/agents/agents_basics/)
*   [Connectors](/agents/connectors/connectors/)

*   [MCP](/agents/mcp/)
*   [Agents Function Calling](/agents/function_calling/)
*   [Agents Handoffs](/agents/handoffs/)
*   Deployment
*   [La Plateforme](/deployment/laplateforme/overview/)

*   [Cloud](/deployment/cloud/overview/)

*   [Self-deployment](/deployment/self-deployment/overview/)

*   Guides
*   [Prompting capabilities](/guides/prompting_capabilities/)
*   [Basic RAG](/guides/rag/)
*   [Prefix](/guides/prefix/)
*   [Tokenization](/guides/tokenization/)
*   [Sampling](/guides/sampling/)
*   [Fine-tuning](/guides/finetuning/)
*   [Evaluation](/guides/evaluation/)
*   [Observability](/guides/observability/)
*   [Other resources](/guides/resources/)
*   [How to contribute](/guides/contribute/overview/)

*   [](/)
*   [Models](/getting-started/models/models_overview/)
*   Models Benchmarks

On this page

# Models Benchmarks

LLM (Large Language Model) benchmarks are standardized tests or datasets used to evaluate the performance of large language models. These benchmarks help researchers and developers understand the strengths and weaknesses of their models and compare them with other models in a systematic way.

## Mistral benchmarks[​](#mistral-benchmarks "Direct link to Mistral benchmarks")

Mistral demonstrates top-tier reasoning capabilities and excels in advanced reasoning, multilingual tasks, math, and code generation. The company reports benchmark results on popular public benchmarks such as MMLU (Massive Multitask Language Understanding), MT-bench, and others.

You can find the benchmark results in the following blog posts:

*   [Magistral Medium](https://mistral.ai/news/magistral/): Our frontier-class reasoning model built on top of Mistral Medium 3, offering state-of-the-art reasoning capabilities for enterprise use cases.
*   [Magistral Small](https://mistral.ai/news/magistral/): Our small reasoning model built on top of Mistral Small 3, a compact open model with high-performance reasoning.
*   [Mistral Medium](https://mistral.ai/news/mistral-medium-3/): A cutting-edge multimodal model. It can be used for professional use cases, coding, function-calling and reasoning tasks.
*   [Pixtral Large](https://mistral.ai/news/pixtral-large/): Pixtral Large is a 124B open-weights multimodal model built on top of Mistral Large 2. It is the second model in our multimodal family and demonstrates frontier-level image understanding.
*   [Pixtral 12B](https://mistral.ai/news/pixtral-12b/): Pixtral 12B the first open-source model to demonstrate state-of-the-art multimodal understanding, without regressing on abilities in pure text.
*   [Mistral Large](https://mistral.ai/news/mistral-large-2407/): A cutting-edge text generation model with top-tier reasoning capabilities. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.
*   [Mistral Small](https://mistral.ai/news/mistral-small-3-1/): A powerful and efficient multimodal model with high level conversational and translation capabilities.
*   [Devstral Small](https://mistral.ai/news/devstral/): A powerful and efficient 24B text model. Debuting as #1 open source model on SWE-bench. This model has high agentic capabilities, performing well with OpenHands and SWE-agent code agent frameworks.
*   [Mistral Saba](https://mistral.ai/news/mistral-saba/): A powerful and efficient text generation model trained extensively on languages from the Middle East and South Asia, achieving state-of-the-art quality comparable to much larger models. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.
*   [Mistral Nemo](https://mistral.ai/news/mistral-nemo/): Mistral Nemo's reasoning, world knowledge, and coding performance are state-of-the-art in its size category. As it relies on standard architecture, Mistral Nemo is easy to use and a drop-in replacement in any system using Mistral 7B that it supersedes.
*   [Codestral](https://mistral.ai/news/codestral-2501/): Codestral sets a new standard on the performance/latency space for code generation compared to previous models used for coding.
*   [Codestral-Mamba](https://mistral.ai/news/codestral-mamba/): we have trained this model with advanced code and reasoning capabilities, enabling the model to have a strong performance on par with SOTA transformer-based models.
*   [Mathstral](https://mistral.ai/news/mathstral/): Mathstral stands on the shoulders of Mistral 7B and specialises in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks.
*   [Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/): our most performant open model. It handles English, French, Italian, German, Spanish and performs strongly on code-related tasks. Natively handles function calling.
*   [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts/): outperforms Llama 2 70B on most benchmarks with 6x faster inference and matches or outperforms GPT3.5 on most standard benchmarks. It handles English, French, Italian, German and Spanish, and shows strong performance in code generation.
*   [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/): outperforms Llama 2 13B on all benchmarks and Llama 1 34B on many benchmarks.

## Scale Seal Leaderboard[​](#scale-seal-leaderboard "Direct link to Scale Seal Leaderboard")

[Scale AI](https://scale.com/leaderboard) reports private benchmark results in coding, instruction following, math, and Spanish. Mistral Large performs exceptionally well in code and Spanish, outperforming Llama 3 405B in these areas.

## Artificial Analysis[​](#artificial-analysis "Direct link to Artificial Analysis")

[Artificial Analysis](https://artificialanalysis.ai/models) compares and evaluates AI models across key performance metrics, including quality, price, output speed, latency, context window, and others. Our model has several areas of outstanding performance worth highlighting.

*   Artificial Analysis Quality Index: Our model ranks 3rd in this benchmark, surpassing even the 405B model. This achievement underscores our model's superior ability to analyze and generate high-quality insights.
*   Coding (HumanEval): In the HumanEval benchmark, our model secures the 3rd position, again outperforming the 405B model. This highlights our model's exceptional proficiency in coding tasks.
*   Quantitative Reasoning (MATH): Our model places 4th in the MATH benchmark, ahead of the 405B model. This demonstrates our model's strong quantitative reasoning capabilities.
*   Scientific Reasoning & Knowledge (GPQA): In the GPQA benchmark, our model ranks 4th, showcasing its robust scientific reasoning and knowledge retention abilities.

## Qualitative feedback[​](#qualitative-feedback "Direct link to Qualitative feedback")

We've gathered a lot of valuable insights from platforms like Reddit and Twitter. Below are some highlights and quotes from users who have shared their experiences with our models.

### Pixtral:[​](#pixtral "Direct link to Pixtral:")

> Pixtral absolutely SLAYS at OCR.

> Very impressive at charts and diagrams and drawings and photos of screens.

> It outperforms GPT-4o-mini in many examples I’ve tested.

### Mistral Large:[​](#mistral-large "Direct link to Mistral Large:")

> Mistral large 2 has been my go to model.

> This model is so good. In terms of local models, this is probably the first that I honestly felt was proprietary tier for coding.

### Mistral Nemo:[​](#mistral-nemo "Direct link to Mistral Nemo:")

> I’ve been playing with Nemo for a few days now, and it blows me away at how coherent it is. It’s slightly ‘less creative and more repetitive’ than Llama 3 8B fine-tunes… But it feels ‘more coherent and has better instruction capabilities’.

> Just wanna say thank you to those genius french over at Mistral for Nemo. 12B parameters and 128k context is a very useful combination. It’s enough of a size improvement over 7B to feel a little more “solid” when talking to it, and it runs circles around Llama-2-13B, with 32x the context length. Thank you mistral!

[

Previous

Models Overview

](/getting-started/models/models_overview/)[

Next

Model selection

](/getting-started/models/picking/)

*   [Mistral benchmarks](#mistral-benchmarks)
*   [Scale Seal Leaderboard](#scale-seal-leaderboard)
*   [Artificial Analysis](#artificial-analysis)
*   [Qualitative feedback](#qualitative-feedback)
*   [Pixtral:](#pixtral)
*   [Mistral Large:](#mistral-large)
*   [Mistral Nemo:](#mistral-nemo)

Documentation

*   [Documentation](/)
*   [Contributing](/guides/contribute/overview/)

Community

*   [Discord](https://discord.gg/mistralai)
*   [X](https://twitter.com/MistralAI)
*   [GitHub](https://github.com/mist